
# knitr document van Steensel lab

# Thethered TRIP data pre-processing
## Christ Leemans, 31-05-2016 - to date 

## Introduction
The current pipeline for Laura's thethered TRIP experiments only considers uniquely reads that can be uniquely mapped. But TRIP intergrations inside repetitive elements might also provide valuable information on how the genome is organized.

In answering the question of how TRIP intergations behave in repetitive elements, I would also like to really dive into the mechanisms of the pipeline. Right now Laura and Eva are using two completely different scripts and in addition Eva still has another script of Wasseem that is his most recent work, but unfortunately it is not in use since there were still some unanswered questions about how to make it work.

## Experimental setup
At this moment Laura has data for 3 different tethering experiments using KRAB, G9a and CBX5. For each protein of interest (POI) there are 12 expression and 12 gDNA files: 3 conditions * 2 different days after induction * 2 replicates. One condition uses an unthethered POI, the second uses only GAL4 and the third condition uses the POI thethered to GAL4 (GAL4-POI). Expression and gDNA data was obtained on day 2 and day 9. With each sequencing run, spikeins were added to normalize across different experiments. There is a different config file to extract the expression values of the spikeins.

## Input types
The input for the TRIP pipeline is made up of 4 different sets of fastq from different sources. These contain gDNA for normalization, cDNA for expression levels, forward iPCR and reverse iPRC reads for mapping the intergrations.

read structure:

**gDNA/cDNA:**
\# index - pat1 - barcode - pat2  
\# [N*10]GTCACAAGGGCCGGCCACAACTCGAG[N*16]TGATCCTGCAGTGTCACCTAAATCGTATGCGGCCGCGAATTCTTACTT

In the config file the following settings are used for these reads:  
* index_length = 10
* barcode_length = 16
* pat1 = GTCACAAGGGCCGGCCACAACTCGAG
* pat2 = TGATC
* min_counts = 3 # amount of times a barcode has to be counted to be considered
* hd = 2 # the max hamming distance between two barcodes for them to still be considered the same

**forward iPCR:**  
\# index - pat1 - barcode - pat2 - gDNA  
\# [N*10]GTCACAAGGGCCGGCCACAACTCGAG[N*16]TGATC[N*43]

In the config file the following settings are used for these reads:  
* index_length = 10
* barcode_length = 16
* map_pat1 = GTCACAAGGGCCGGCCACAACTCGAG
* map_pat2 = TGATC
* max_dist_for = 500 # two forward iPCR reads mapped less than 500bp apart are considered the same intergration site

**reverse iPCR:**  
\# map_pat_rev - gDNA  
\# GTACGTCACAATATGATTATCTTTCTAGGGTTAA[N*66]

In the config file the following settings are used for these reads:  
* map_pat_rev = GTACGTCACAATATGATTATCTTTCTAGGGTT
* max_dist_for = 50 # two reverse iPCR reads mapped less than 50bp apart are considered the same intergration site



## TRIP pipeline

With my new version of the trip pipeline with the following commands I could get the expression and normalization values as well as the genomic intergration positions for each file:

```shell
## normalization and expression values for the experimental conditions
nice -19 ~/python/bin/python src/python/trip.py -o cl20160815_trip_without_bc_list -c cl20160816_config_K562_TTRIP.txt -l cl20160816_norm_exp_file_list.lst -u -v -d 2>&1 | tee cl20160815_trip_without_bc_list/norm_exp.stdout.stderr.txt

## expression values for the spike-in library
nice -19 ~/python/bin/python src/python/trip.py -o cl20160815_trip_spikein -c cl20160602_config_spikein_K562_TTRIP.txt -l cl20160816_spike_file_list.lst -u -v -d 2>&1 | tee cl20160815_trip_spikein/norm_exp.stdout.stderr.txt

## we are only interested in finding the intergration site of a barcode if, for a single experiment, normalization reads are found in each replicate
tail -n+2 cl20160815_trip_without_bc_list/bc_count.txt | awk '{print NR"\t"$1}' > cl20160815_trip_without_bc_list/bc_table.txt

## find the intergration sites
nice -19 ~/python/bin/python src/python/trip.py -o cl20160815_trip_without_bc_list -c cl20160816_config_K562_TTRIP.txt -b cl20160815_trip_without_bc_list/bc_table.txt -l cl20160816_mapping_file_list.lst -m b -u -v -d 2>&1 | tee cl20160815_trip_without_bc_list/mapping.stdout.stderr.txt

## sort the bed file of all mapping locations
bedtools sort -i cl20160906_TTRIP_K562/mapping/rev_mapping.bed > cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed 

## combine the two sam files with reverse reads so that remapped reads are replaced in samRev.sam
nice -15 awk 'FNR==NR{if ($1 ~ /^@/) {print $0; next} else {arr[$1]=$0};next}{if ($1 in arr){print arr[$1]}else{print $0}}' cl20160815_trip_without_bc_list/samRev2.sam cl20160815_trip_without_bc_list/samRev.sam | samtools view -Sb - > cl20160815_trip_without_bc_list/samRev_combined.bam

```
### Output:
- samFor.sam			# forward reads mapped to genome  
- samRev.sam            # reverse reads mapped to genome  
- samRev2.sam           # substrings of reads that needed remapping
- samRev_combined.bam   # combined reverse reads   
- bc_count.txt          # barcode counts  
- bc_table.txt			# barcode table for mapping
- 3354_1_iPCR_laura_eva_altndx_R1_001_smplIdx_[09-14]_fwd1.fastq (6 files)  
						# 6 fastqs with just the gDNA of forward reads (see read structure) and barcode in sequence id  
- 3354_1_iPCR_laura_eva_altndx_R1_001_smplIdx_[09-14]_rev1.fastq (6 files)  
						# 6 fastqs with just the gDNA of reverse reads (see read structure) and barcode in sequence id  
- final_mapping.txt     # mapped barcode locations
- stats.txt 			# stats.txt


## Linking barcodes to repetitive elements
For each read we have a barcode and a location, this location can be overlapping with a repetitive element. First all reads need to be matched with repetitive elements, then the barcode of that read can be linked. One barcode will have multiple reads and also repetitive elements. In the end we want a count of how many times a barcode has been linked to a type of repetitive element.
```

awk '{if ($1!="*") {print $0}}' cl20160815_trip_without_bc_list/rev_mapping.bed | bedtools intersect -wa -wb -a - -b /home/NFS/users/c.leemans/data/tracks/hg19/repeatMasker_hg19_fa_out_20140131.bed | \
awk -F"[|\t]" '
BEGIN {
print "barcode\tclass\tfamily\tname\tcount\ttotal"
}{
  if($10 ~ /\//){
    match($10,/(.*)\/(.*)/, a)
    class=a[1]
    fam=a[2]
  } else{
    class=$10
    fam="-"
  }
  i=($4"\t"class"\t"fam"\t"$11)
  count[i] += $5
  total[i] = $6
}END {
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160815_trip_without_bc_list/bc_in_repeats_samRev.txt

```


## Linking barcodes to LAD states
I found an rData file of Caroline with LAD states for different cell types from microarray experiments. In this rdata file is a table with binarized LAD information (1 for iLAD, 2 for cLAD) called 'allHumanStateHg19'. There is also a table called 'allHumanAvHg19' with raw peaks and a table called cLADhs where constitutive LAD states are defined.

For now I am using only the allHumanStateHg19 table. 4 different 'states' can be described in the context of the K562 thethered-TRIP expiriment: constitutive LADs (cLADs), constitutive inter-LADs (ciLADs), facultative LADs (fLADs) which are associated with the lamina in K562 cell line and fLADs which are not associated with the lamina in K562 cell line, but with different cell types.

```r

load('/home/NFS/users/ca.d.graaf/projects/analyses/CdG121011humanLmnb1Atlas/CdG140714humanLmnb1wK562.rData')
## the average probe signal can also be used
write.table(allHumanAvHg19[,c('seqname', 'start', 'end', 'K5')], file='cl20161017_LAD_avg_probe_K562.bed',col.names=F,quote=F,row.names=F, sep='\t')

lad_2state = allHumanStateHg19$K6
lad_2state[lad_2state==1] = 'interLAD'
lad_2state[lad_2state==2] = 'LAD'
write.table(cbind(allHumanStateHg19[,c('seqname', 'start', 'end')], lad_2state), file='cl20161019_LAD_2state_K562.bed',col.names=F,quote=F,row.names=F, sep='\t')

## let's not select the down syndrome data and the TIG3 that was done on another array
cell_type = colnames(allHumanStateHg19)[5:16]
cell_type = cell_type[!cell_type%in%c('DS', 'FB', 'TN')]

ciLAD = length(cell_type)
cLAD = ciLAD * 2

allHumanStateHg19$LAD = 'ciLAD'
allHumanStateHg19$LAD[rowSums(allHumanStateHg19[,cell_type])==cLAD] = 'cLAD'
allHumanStateHg19$LAD[rowSums(allHumanStateHg19[,cell_type])<cLAD&rowSums(allHumanStateHg19[,cell_type])>ciLAD&allHumanStateHg19$K6==2] = 'fLAD'
allHumanStateHg19$LAD[rowSums(allHumanStateHg19[,cell_type])<cLAD&rowSums(allHumanStateHg19[,cell_type])>ciLAD&allHumanStateHg19$K6!=2] = 'fiLAD'


write.table(allHumanStateHg19[,c('seqname', 'start','end','LAD')],file = 'cl20161017_LAD_4state_K562.bed', col.names = F, quote=F,row.names=F) 

## count probes linked to each state
table(allHumanStateHg19$LAD)

```
**probe counts for each state:**

|  ciLAD |   cLAD |      fLAD |      fiLAD |
|:------:|:------:|:---------:|:----------:|
| 434601 | 431384 |    474769 |    793625  |

Now that we have the probe locations with states in a bed file, we need to translate this to regions for each state. Otherwise we can only match insertions that took place at the exact probe sites.

```
## 2-state
cat cl20161019_LAD_2state_K562.bed | awk '{if (NR==1){chr=$1; start=$2; end=$3; lad=$4} else if ($4==lad&&$1==chr){end=$3} else {print chr"\t"start"\t"end"\t"lad; chr=$1; start=$2; end=$3; lad=$4}}END{print chr"\t"start"\t"end"\t"lad}' > cl20161019_LAD_continuous_2state_K562.bed 

## LAD-borders
awk '{
if (NR!=1&&chr==$1){
  print chr"\t"start"\t"$2
}
start=$3
chr=$1
}' cl20161019_LAD_continuous_2state_K562.bed > cl20161019_LAD_borders_K562.bed

## 4-state
cat cl20161017_LAD_4state_K562.bed | awk '{if (NR==1){chr=$1; start=$2; end=$3; lad=$4} else if ($4==lad&&$1==chr){end=$3} else {print chr"\t"start"\t"end"\t"lad; chr=$1; start=$2; end=$3; lad=$4}}END{print chr"\t"start"\t"end"\t"lad}' > cl20161017_LAD_continuous_4state_K562.bed 

## count number of regions linked to each state
cat cl20161017_LAD_continuous_4state_K562.bed | sort -k4,4 | uniq -c -f 3 | awk '{print $5"\t"$1}'

```
**number of regions for each state:**

|  ciLAD |   cLAD |      fLAD |      fiLAD |
|:------:|:------:|:---------:|:----------:|
|   2371 |   3250 |      5491 |       5052 |

With this file, the barcodes can be linked:

```

awk '{if ($1!="*") {print $0}}' cl20160815_trip_without_bc_list/rev_mapping.bed | nice -18 bedtools intersect -a - -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161017_LAD_continuous_4state_K562.bed -wa -wb | awk 'BEGIN {
print "barcode\tlad\tcount\ttotal"
}{
  count[$4"\t"$10] += $5
  total[$4"\t"$10] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160815_trip_without_bc_list/barcode_LAD_state.txt

nice -18 bedtools intersect -a cl20160906_TTRIP_K562/mapping/rev_mapping.bed -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161019_LAD_continuous_2state_K562.bed -wa -wb | awk 'BEGIN {
print "barcode\tlad\tlad_size\tcount\ttotal"
}{
  count[$4"\t"$10"\t"$9-$8] += $5
  total[$4"\t"$10"\t"$9-$8] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_lad_2state.txt

bedtools closest -io -t all -d -a cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161019_LAD_borders_K562.bed | awk 'BEGIN {
print "barcode\tdistance\tcount\ttotal"
}{
  count[$4"\t"$10] += $5
  total[$4"\t"$10] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_lad_border.txt
```


## linking barcodes to chromatin state
I recieved a bed file with the annotations of chromatin states from Laura called 'wgEncodeBroadHmmK562HMM.bed'. I could use a similar command to the previous barcode linking steps:

```
awk '{if ($1!="*") {print $0}}' cl20160815_trip_without_bc_list/rev_mapping.bed | nice -18 bedtools intersect -a - -b /home/NFS/users/c.leemans/data/tracks/hg19/wgEncodeBroadHmmK562HMM.bed -wa -wb | awk 'BEGIN {
print "barcode\tchrom\tcount\ttotal"
}{
  count[$4"\t"$10] += $5
  total[$4"\t"$10] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160815_trip_without_bc_list/barcode_chrom_state.txt

```

## linking barcodes to HiC compartments
I used GM12878 compartment calls from the [Rao et al. 2014 cell](http://dx.doi.org/10.1016/j.cell.2014.11.021) paper.
It would be too cumbersome to repeat this compartment calling on K562 cells, although the HiC data was also published in the same paper.
But both GM12878 as well as K562 cell lines originated from blood cells and the data should be highly similar.

```
bedtools intersect -a cl20160906_TTRIP_K562/mapping/rev_mapping.bed -b ~/data/tracks/hg19/GSE63525_GM12878_subcompartments.bed -wa -wb | awk '
BEGIN {
print "barcode\tcompartment\tcount\ttotal"
}{
  count[$4"\t"$10] += $5
  total[$4"\t"$10] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' >  cl20160906_TTRIP_K562/bc_subcompartments.txt
```

## linking barcodes to replication timing

```

for state in $(echo 'G1 S1 S2 S3 S4 G2')
do
  nice -15 awk '{print $1"\t"$2"\t"$3"\t"$4"_"$5"/"$6}' cl20160815_trip_without_bc_list/rev_mapping.bed | bwtool extract bed /dev/stdin '/home/NFS/users/c.leemans/data/tracks/hg19/GSM923448/GSM923448_hg19_wgEncodeUwRepliSeqK562'$state'PctSignalRep1.bigWig'  /dev/stdout | awk -F'[,_\t|/]' 'BEGIN{
    print "barcode\tsignal\tcount\ttotal"
  }{
    i=$4"\t"($8 + $9 + $10 + $11)/4
    count[i] += $5
    total[i] = $6
  }END{
    for (i in count){
      print i"\t"count[i]"\t"total[i]
    }
  }' > 'cl20160815_trip_without_bc_list/replication_'$state'.txt' &
done


awk '{if ($1!="*") {print $0}}' cl20160815_trip_without_bc_list/rev_mapping.bed | nice -18 bedtools intersect -a - -b /home/NFS/users/c.leemans/data/tracks/hg19/GSE53984_GSM923448_K562_Rep1_segments.bed -wa -wb | awk 'BEGIN {
print "barcode\tsegment\tcount\ttotal"
}{
  count[$4"\t"$10] += $5
  total[$4"\t"$10] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160815_trip_without_bc_list/barcode_rep_segments.txt


bedtools sort -i cl20160906_TTRIP_K562/mapping/rev_mapping.bed | bedtools window -c -w 2000 -a - -b ~/data/tracks/hg19/K562_CBX8_CHIP_ENCFF001SYX.bed | awk 'BEGIN {
  print "barcode\tin_region\tcount\ttotal"
}{
  count[$4"\t"$7] += $5
  total[$4"\t"$7] = $6
}END{
  for (i in count){
     print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_CBX8.txt

bedtools sort -i cl20160906_TTRIP_K562/mapping/rev_mapping.bed | bedtools window -c -w 5000 -a - -b ~/data/tracks/hg19/K562_CBX5_ENCFF507SAT.bed | awk 'BEGIN {
  print "barcode\tin_region\tcount\ttotal"
}{
  count[$4"\t"$7] += $5
  total[$4"\t"$7] = $6
}END{
  for (i in count){
     print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_CBX5.txt

bedtools sort -i cl20160906_TTRIP_K562/mapping/rev_mapping.bed | bedtools window -c -w 5000 -a - -b ~/data/tracks/hg19/K562_CBX1_CHIP_ENCFF182ALR.bed | awk 'BEGIN {
  print "barcode\tin_region\tcount\ttotal"
}{
  count[$4"\t"$7] += $5
  total[$4"\t"$7] = $6
}END{
  for (i in count){
     print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_CBX1.txt

```

## finding nearest CpG island

```
awk '{if ($1!="*") print $0}' cl20160815_trip_without_bc_list/rev_mapping.bed | nice -18 bedtools sort -i | awk '{if($1!="*"){print $0}}' | bedtools closest -t all -D a -a - -b /home/NFS/users/c.leemans/data/tracks/hg19/cpgIslandExtUnmasked_140601.bed | awk 'BEGIN {
print "barcode\tCpG_name\tdistance\tcount\ttotal"
}{
  count[$4"\t"$10"\t"$11] += $5
  total[$4"\t"$10"\t"$11] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160815_trip_without_bc_list/cpg_distance.txt

```

## finding nearest CTCF

```


bedtools sort -i /home/NFS/users/c.leemans/data/tracks/hg19/K562_CTCF_CHIP_ENCFF002CLT.bed | bedtools closest -t all -D a -a cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed -b - | awk 'BEGIN {
print "barcode\tCTCF\tdistance\tcount\ttotal"
}{
  count[$4"\t"$10"\t"$11] += $5
  total[$4"\t"$10"\t"$11] = $6
}END{
  for (i in count){
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_ctcf_distance.txt


```

## distance to telomere

```
bedtools closest -t all -D a -a cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161004_telomere.bed | awk 'BEGIN {
print "barcode\tdistance\tcount\ttotal"                     
}{
  count[$4"\t"$11] += $5                                       
  total[$4"\t"$11] = $6                                        
}END{                                                                 
  for (i in count){                                                   
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_telomere_distance.txt


```


## distance to centromere

```
bedtools closest -t all -D a -a cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161004_centromere.bed | awk 'BEGIN {
print "barcode\tdistance\tcount\ttotal"                     
}{
  count[$4"\t"$11] += $5                                       
  total[$4"\t"$11] = $6                                        
}END{                                                                 
  for (i in count){                                                   
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_centromere_distance.txt


```


## distance to centromere

```
bedtools closest -t all -D a -a cl20160906_TTRIP_K562/mapping/rev_mapping_sorted.bed -b /home/NFS/users/c.leemans/data/tracks/hg19/cl20161004_UCSC_knownGene_20130630.bed | awk 'BEGIN {
print "barcode\tdistance\tcount\ttotal"                     
}{
  count[$4"\t"$11] += $5                                       
  total[$4"\t"$11] = $6                                        
}END{                                                                 
  for (i in count){                                                   
    print i"\t"count[i]"\t"total[i]
  }
}' > cl20160906_TTRIP_K562/bc_gene_distance.txt


```

## average signals 50kb around integration
hg19.chrom.sizes was obtained using the fetchChromSizes script from the UCSC browser

```
awk '
FNR==NR {
  sizes[$1] = $2
}
FNR!=NR {
  start=$2-25000
  if (start<0){
    start=0
  }
  end=$3+25000
  if (end > sizes[$1]){
    end=sizes[$1]
  }
  print $1"\t"start"\t"end"\t"$4"\t"$5"\t"$6
}' ~/data/hg19/hg19.chrom.sizes cl20160906_TTRIP_K562/mapping/rev_mapping.bed > cl20160906_TTRIP_K562/mapping/rev_mapping_50kb.bed


bedtools intersect -a cl20160906_TTRIP_K562/mapping/rev_mapping_50kb.bed -b ~/data/tracks/hg19/cl20161017_LAD_avg_probe_K562.bed -wa -wb | awk '
BEGIN {
print "barcode\tavg_signal\tcount\ttotal"
}{
  i=($4"\t%s\t"$5"\t"$6"\n")
  sum_signal[i] += $10
  probe_count[i] += 1
} END {
  for (i in sum_signal){
    printf i, sum_signal[i]/probe_count[i]
  }
}'  > cl20160906_TTRIP_K562/bc_avg_probe_signal.txt

~/python3.4/bin/python src/snakemake/scripts/AT_richness.py cl20160906_TTRIP_K562/mapping/rev_mapping_50kb.bed ~/data/hg19/genome.fa > cl20160906_TTRIP_K562/bc_lad_avg_probe.txt

```


# R analysis
Now that we have barcodes linked to gDNA and cDNA counts and barcodes linked to repetitive element annotations, LAD states and chromatin states, we can combine this in R to calculate expression values and fold changes and produce some nice plots.


## Path, Libraries, Parameters and Useful Functions

```{r, cache=F}
StartTime <-Sys.time()

# 6-digit Date tag:
Date <- substr(gsub("-","",Sys.time()),3,8) 

# libraries:
library(stringr)
library(ggplot2)
library(reshape2)
library(knitr)
library(gridExtra)
library(plyr)
library(doMC)

## for faster ddply functions
registerDoMC(2)

path = '/home/c.leemans/SURFdrive/TRIP'

```



## Load data
First we need to load the data and have some description of which column contains which information

```{r}
counts_without_bc = read.table('../cl20160815_trip_without_bc_list/bc_count.txt', stringsAsFactors=F, header=T, row.names=1)
spike_counts = read.table('../cl20160815_trip_spikein/bc_count.txt', stringsAsFactors=F, header=T, row.names=1)
spike_counts = spike_counts[spike_counts[,1]>1000, 2:ncol(spike_counts)]
mapping = read.table('../cl20160906_TTRIP_K562/mapping/final_mapping.txt', header=TRUE, row.names=1)

bc_repeats = read.table('../cl20160906_TTRIP_K562/bc_repeat.tmp', stringsAsFactors=F, header=T)

bc_lad = read.table('../cl20160906_TTRIP_K562/bc_lad.txt',stringsAsFactors=F, header=T)
bc_lad_2state = read.table('../cl20160906_TTRIP_K562/bc_lad_2state.txt',stringsAsFactors=F, header=T)
bc_lad_border = read.table('../cl20160906_TTRIP_K562/bc_lad_border.txt',stringsAsFactors=F, header=T)
bc_subcompartments = read.table('../cl20160906_TTRIP_K562/bc_subcompartments.txt',stringsAsFactors=F, header=T)

bc_rep_segments = read.table('../cl20160906_TTRIP_K562/bc_rep_segments.txt',stringsAsFactors=F, header=T)

bc_chromatin = read.table('../cl20160906_TTRIP_K562/bc_chrom.txt', stringsAsFactors=F, header=T)
timing_fases = c('G1', 'S1', 'S2', 'S3', 'S4', 'G2')
bc_timing = lapply(timing_fases,
                   function(x){
                    file_name = sprintf('../cl20160906_TTRIP_K562/bc_timing_%s.txt', x)
                    timing = read.table(file_name, stringsAsFactors=F, header=T)
                    colnames(timing)[colnames(timing)=='signal'] = sprintf('signal_%s', x)
                    return(timing)
})
names(bc_timing) = timing_fases
bc_cpg_distance = read.table('../cl20160906_TTRIP_K562/bc_cpg_distance.txt', stringsAsFactors=F, header=T)
bc_ctcf_distance = read.table('../cl20160906_TTRIP_K562/bc_ctcf_distance.txt', stringsAsFactors=F, header=T)
bc_centromere_distance = read.table('../cl20160906_TTRIP_K562/bc_centromere_distance.txt', stringsAsFactors=F, header=T)
bc_telomere_distance = read.table('../cl20160906_TTRIP_K562/bc_telomere_distance.txt', stringsAsFactors=F, header=T)
bc_gene_distance = read.table('../cl20160906_TTRIP_K562/bc_gene_distance.txt', stringsAsFactors=F, header=T)
bc_at_ratio = read.table('../cl20160906_TTRIP_K562/bc_at_ratio.txt', stringsAsFactors=F, header=T)
bc_lad_avg_probe = read.table('../cl20160906_TTRIP_K562/bc_lad_avg_probe.txt', stringsAsFactors=F, header=T)

bc_CBX1 = read.table('../cl20160906_TTRIP_K562/bc_CBX1.txt',stringsAsFactors=F, header=T)
bc_CBX5 = read.table('../cl20160906_TTRIP_K562/bc_CBX5.txt',stringsAsFactors=F, header=T)
bc_CBX8 = read.table('../cl20160906_TTRIP_K562/bc_CBX8.txt',stringsAsFactors=F, header=T)
bc_CBX3_000 = read.table('../cl20160906_TTRIP_K562/bc_CBX3_000.txt',stringsAsFactors=F, header=T)
bc_CBX3_163 = read.table('../cl20160906_TTRIP_K562/bc_CBX3_163.txt',stringsAsFactors=F, header=T)

info = data.frame(POI=rep(c(rep(c('G9a','CBX','KRAB'), each=12), rep(c('CBX', 'KRAB'), each=6)), 2),
                  condition=rep(c(rep(c('GAL4', 'GAL4.POI', 'POI'),4), rep(c('GAL4.POI', 'GAL4', 'POI'),4),rep(c('GAL4','GAL4.POI',  'POI'),4),rep(c('GAL4.POI', 'GAL4', 'POI'),each=2), rep(c('GAL4', 'GAL4.POI','POI'), each=2)),2),
                  type=rep(c('norm', 'exp'),each=48),
                  day=rep(c(2,12,2,9,2,11,12,14), each=6, 2),
                  replicate=rep(c(rep(c(1,2),each=3, 6), rep(c(1,2), 6)), 2), stringsAsFactors=F)


colnames(counts_without_bc) = do.call(sprintf, c('%s_%s_%s_D%s_r%s', info))
colnames(spike_counts) = do.call(sprintf, c('%s_%s_%s_D%s_r%s',info[info$type=='exp',]))
```

<!-- ## distribution of the reads accros the different barcodes.
Let's see how the barcode counts are distributed in each of the datasets.

```{r hist_count, fig.width=10, fig.height=80}
## first let's see the general picture
for (poi in unique(info$POI)){
  for (day in unique(info$day[info$POI==poi])){
    for (condition in unique(info$condition)){
      plot_list = list()
      for (type in unique(info$type)){
        for (rep in unique(info$replicate)){
          name = colnames(counts_without_bc)[info$POI==poi & info$day == day &
                                             info$condition == condition &
                                             info$type == type &
                                             info$replicate == rep]

          plot_list[[name]] = ggplot(counts_without_bc[counts_without_bc[,name] > 5, ], aes_string(paste0('log2(', name, ')')))+geom_histogram(binwidth=0.1)
        }
      }
      do.call(grid.arrange, c(plot_list, ncol=2))
    }
  }
}

## and now the barcodes for which we have mapping data
for (poi in unique(info$POI)){
  for (day in unique(info$day[info$POI==poi])){
    for (condition in unique(info$condition)){
      plot_list = list()
      for (type in unique(info$type)){
        for (rep in unique(info$replicate)){
          name = colnames(counts_without_bc)[info$POI==poi & info$day == day &
                                             info$condition == condition &
                                             info$type == type &
                                             info$replicate == rep]
          print(name)
          selection = counts_without_bc[,name]>5 & rownames(counts) %in% rownames(mapping)
          plot_list[[name]] = ggplot(counts_without_bc[selection, ], aes_string(paste0('log2(', name, ')')))+geom_histogram(binwidth=0.1)
        }
      }
      do.call(grid.arrange, c(plot_list, ncol=2))
    }
  }
}

```

## Some data pre-processing -->

We have to link each barcode to a unique LAD/chromatin state and repetitive element and need to normalize expression values by the total amount of spike-in reads for each sample.

### unique barcode links

first let's see how many barcodes have multiple hits with different states/repeats:

```{r, results='asis'}

plot1 = ggplot(as.data.frame(table(bc_repeats$barcode)),aes(x=Freq)) + geom_histogram(binwidth = 1) + xlim(0,10) + xlab('number of unique repetitive elements')
plot2 = ggplot(as.data.frame(table(bc_lad$barcode)),aes(x=Freq)) + geom_histogram(binwidth = 1) + xlim(0,10) + xlab('number of unique lad states')
plot3 = ggplot(as.data.frame(table(bc_chromatin$barcode)),aes(x=Freq)) + geom_histogram(binwidth = 1) + xlim(0,10) + xlab('number of unique\nchromatin states')

# grid.arrange(plot1, plot2,plot3,top = "count of barcodes linked to each number of states",nrow=1)

stat_bc_link = data.frame('ratio'=c('barcodes linked to repeats'=length(table(bc_repeats$barcode)), 'barcodes linked to lads'=length(table(bc_lad$barcode)), 'barcodes linked to chromatin states'=length(table(bc_chromatin$barcode))))

stat_bc_link$ratio = c(length(which(table(bc_repeats$barcode)==1)), length(which(table(bc_lad$barcode)==1)), length(which(table(bc_chromatin$barcode)==1))) / stat_bc_link$ratio

kable(stat_bc_link)

```
**Conclusion:**
The number of barcodes having a single, unique element/state linked to them are in for each of the 3 types fairly high. For repetitive elements, this number is lower, but there are also less barcodes in general so in the end the ratio of uniquely mapped barcodes is the highest.

These barcodes are unfiltered for multiple [mapping] locations, so after removing barcodes with multiple mapping locations, these numbers should get better. 


### select most abundant barcode link
For each barcode we would ideally want a single repeat/state. But we want to know if there are alternative states/repeats found. To decide wether a barcode is linked to a unique state/repeat I am using the same threshold as Laura used for deciding on unique mapping locations: the most abundant must be occuring in a ratio of at least 0.7 with the other hits and the second hit in a ratio less than 0.1.

For the barcodes linked to repeats I decided to add a seperate overview for barcodes linked to unique repeat family's instead of unique names, since there are only minor differences between repeat names in the same family.

```{r, cache=F}
single_bc_link <- function(this_link, link_type){
  # function to obtain a table with a single link with a state for each barcode
  # in addition a column is added with a boolean for whether the threshold
  # for uniqueness is passed.
  hit_order = order(this_link$count, decreasing=T)
  hit_1 = this_link[hit_order[1],]
  is_unique = hit_1['count']>3 & hit_1['count']/hit_1['total'] >0.7

  if (length(hit_order)>1){
    hit_2 = this_link[hit_order[2],]
    is_unique = is_unique & hit_2['count']/hit_2['total'] < 0.2
  }
  result = cbind(hit_1, is_unique)
  names(result)[length(result)] = paste0('unique_',link_type)
  return(result)
}

## this method is faster if both run on one core, but a lot less straightforward...
# single_bc_link2 <- function(x, bc_link, link_type){

  
#   # function to obtain a table with a single link with a state for each barcode
#   # in addition a column is added with a boolean for whether the threshold
#   # for uniqueness is passed.
#   this_link = bc_link[x,]
#   hit_order = order(this_link$count, decreasing=T)
#   hit_1 = this_link[hit_order[1],]
#   is_unique = hit_1['count']>30 & hit_1['count']/hit_1['total'] >0.9

#   if (length(hit_order)>1){
#     hit_2 = this_link[hit_order[2],]
#     is_unique = is_unique & hit_2['count']/hit_2['total'] < 0.025
#   }
#   result = c(hit_1[link_type], hit_1['count'], is_unique)
#   names(result)[3] = paste0('unique_',link_type)
#   return(result)
# }
# single_chromatin = aggregate(1:nrow(bc_chromatin),list(bc_chromatin$barcode), single_bc_link2, bc_chromatin, 'chrom')
# single_chromatin = do.call(data.frame, single_chromatin)
# colnames(single_chromatin)[1] = 'barcode'
# colnames(single_chromatin)[2:ncol(single_chromatin)] = gsub('x.', '', colnames(single_chromatin)[2:ncol(single_chromatin)])

single_bc_rep_name = ddply(bc_repeats, .(barcode, class, family, total), single_bc_link, 'rep_name', .parallel=F)

bc_rep_family = aggregate(list(count=bc_repeats$count), bc_repeats[, c('barcode','class', 'family', 'total')],sum)
single_bc_rep_family = ddply(bc_rep_family, .(barcode, class, total),single_bc_link, 'rep_family', .parallel=F)

bc_rep_class = aggregate(list(count=bc_repeats$count), bc_repeats[, c('barcode','class', 'total')],sum)
single_bc_rep_class = ddply(bc_rep_class, .(barcode, total),single_bc_link, 'rep_class', .parallel=F)

single_bc_lad = ddply(bc_lad, .(barcode, total),single_bc_link, 'lad', .parallel=F)
single_bc_lad_2state = ddply(bc_lad_2state, .(barcode, total),single_bc_link, 'lad', .parallel=F)
single_bc_lad_size = ddply(bc_lad_2state, .(barcode, total),single_bc_link, 'size', .parallel=F)
single_bc_lad_border = ddply(bc_lad_border, .(barcode, total),single_bc_link, 'distance', .parallel=F)
single_bc_subcompartments = ddply(bc_subcompartments, .(barcode, total),single_bc_link, 'compartment', .parallel=F)


single_bc_rep_segments = ddply(bc_rep_segments, .(barcode, total),single_bc_link, 'segment', .parallel=F)

single_bc_chrom = ddply(bc_chromatin, .(barcode, total),single_bc_link, 'chrom', .parallel=F)

single_bc_cpg_distance = ddply(bc_cpg_distance, .(barcode, total),single_bc_link, 'cpg_distance', .parallel=F)
single_bc_ctcf_distance = ddply(bc_ctcf_distance, .(barcode, total),single_bc_link, 'ctcf_distance', .parallel=F)

single_bc_centromere_distance = ddply(bc_centromere_distance, .(barcode, total),single_bc_link, 'centromere_distance', .parallel=F)
single_bc_telomere_distance = ddply(bc_telomere_distance, .(barcode, total),single_bc_link, 'telomere_distance', .parallel=F)
single_bc_gene_distance = ddply(bc_gene_distance, .(barcode, total),single_bc_link, 'gene_distance', .parallel=F)
single_bc_at_ratio = ddply(bc_at_ratio, .(barcode, total),single_bc_link, 'AT_ratio', .parallel=F)
single_bc_lad_avg_probe = ddply(bc_lad_avg_probe, .(barcode, total),single_bc_link, 'avg_signal', .parallel=F)

single_bc_CBX1 = ddply(bc_CBX1, .(barcode, total),single_bc_link, 'CBX1', .parallel=F)
single_bc_CBX5 = ddply(bc_CBX5, .(barcode, total),single_bc_link, 'CBX5', .parallel=F)
single_bc_CBX8 = ddply(bc_CBX8, .(barcode, total),single_bc_link, 'CBX8', .parallel=F)
single_bc_CBX3_000 = ddply(bc_CBX3_000, .(barcode, total),single_bc_link, 'CBX3_000', .parallel=F)
single_bc_CBX3_163 = ddply(bc_CBX3_163, .(barcode, total),single_bc_link, 'CBX3_163', .parallel=F)

colnames(single_bc_CBX1)[2] = 'CBX1'
colnames(single_bc_CBX5)[2] = 'CBX5'
colnames(single_bc_CBX8)[2] = 'CBX8'
colnames(single_bc_CBX3_000)[2] = 'CBX3_000'
colnames(single_bc_CBX3_163)[2] = 'CBX3_163'



single_bc_timing = list()
for (i in 1:length(bc_timing)){
  single_bc_timing[[timing_fases[i]]] = ddply(bc_timing[[i]], .(barcode, total), single_bc_link, colnames(bc_timing[[i]])[2], .parallel=F)
}

unique_repeat_name_count = ddply(single_bc_rep_name, .(unique_rep_name), nrow)
unique_repeat_fam_count = ddply(single_bc_rep_family, .(unique_rep_family), nrow)
unique_repeat_class_count = ddply(single_bc_rep_class, .(unique_rep_class), nrow)
unique_lad_count = ddply(single_bc_lad, .(unique_lad), nrow)
unique_lad_2state_count = ddply(single_bc_lad_2state, .(unique_lad_2state), nrow)
unique_rep_segment = ddply(single_bc_rep_segments, .(unique_segment), nrow)
unique_chrom_count = ddply(single_bc_chrom, .(unique_chrom), nrow)
unique_cpg_count = ddply(single_bc_cpg_distance, .(unique_cpg_distance), nrow)
unique_ctcf_count = ddply(single_bc_ctcf_distance, .(unique_ctcf_distance), nrow)
unique_telomere_count = ddply(single_bc_telomere_distance, .(unique_telomere_distance), nrow)
unique_centromere_count = ddply(single_bc_centromere_distance, .(unique_centromere_distance), nrow)
unique_gene_count = ddply(single_bc_gene_distance, .(unique_gene_distance), nrow)
unique_bc_at_ratio = ddply(single_bc_at_ratio, .(unique_AT_ratio), nrow)
unique_bc_avg_probe = ddply(single_bc_lad_avg_probe, .(unique_avg_signal), nrow)
unique_CBX1_count = ddply(single_bc_CBX1, .(unique_CBX1), nrow)
unique_CBX5_count = ddply(single_bc_CBX5, .(unique_CBX5), nrow)
unique_CBX8_count = ddply(single_bc_CBX8, .(unique_CBX8), nrow)
unique_CBX3_000_count = ddply(single_bc_CBX3_000, .(unique_CBX3_000), nrow)
unique_CBX3_163_count = ddply(single_bc_CBX3_163, .(unique_CBX3_163), nrow)

unique_timing = list()
for (i in 1:length(bc_timing)){
  name = timing_fases[i]
  unique_timing[[name]] = ddply(single_bc_timing[[name]], paste0('unique_signal_', name), nrow)
}


```

## Let's make a selection of unique mapping locations

```{r, cache=F}

mapping$unique_map = F
isUnique = which(mapping$t_reads_f>2 & mapping$t_reads_r>2 & mapping$freq1_f>0.7 &mapping$freq1_r>0.7 & mapping$freq2_f<0.1 & mapping$freq2_r<0.1 & mapping$mapq_f>=10 &mapping$mapq_r>=10)
mapping[isUnique,'unique_map'] = T

```



## Normalization of expression
Before we normalize by normalization count, let's make sure that in each replicate counts are normalized by spike-in


```{r, cache=F, fig.width=15, fig.height=6}
sum_spike_counts = colSums(spike_counts)
exp_info = info[info$type=='exp',]
exp_norm_counts = t(t(counts_without_bc[,info$type=='exp'])/sum_spike_counts * 1000000)


norm_counts =  counts_without_bc[,info$type=='norm']

poi_cpm <- function(poi,norm_counts, info){
    cpm = do.call(cbind, lapply(unique(info$replicate),
		function(rep, poi, norm_counts, info){
			cpm = do.call(cbind, lapply(unique(info$day[info$POI==poi&info$replicate==rep]), 
				function(day,rep,poi,norm_counts, info){
					poi = which(info$POI==poi & info$replicate==rep & info$day==day)
				    below_cut = rowSums(norm_counts[,poi]>=1) != length(poi)
				    norm_counts[below_cut,poi] = NA
				    norm_sum = colSums(norm_counts[,poi], na.rm=T)/ 1000000
				    cpm = t(t(norm_counts[,poi]) / norm_sum)
				    return(cpm)
				}, rep, poi, norm_counts, info))
			return(cpm)
		}, poi, norm_counts, info))
    return(cpm[,colnames(norm_counts[,info$POI==poi])])
}
## filter normalization counts so that norm > 1 and counts per million
norm_cpm =  do.call(cbind,lapply(unique(info$POI), poi_cpm, counts_without_bc[,info$type=='norm'], info[info$type=='norm',]))


exp_gDNA = exp_norm_counts/norm_cpm


# for (poi in unique(exp_info$POI)){
#   for (day in unique(exp_info$day[exp_info$POI==poi])){
#     for (condition in unique(exp_info$condition)){
#       plot_list = list()
#       for (type in unique(exp_info$type)){
#         for (rep in unique(exp_info$replicate)){
#           name = colnames(exp_gDNA)[exp_info$POI==poi & exp_info$day == day &
#                                              exp_info$condition == condition &
#                                              exp_info$type == type &
#                                              exp_info$replicate == rep]
#           selection = exp_gDNA[,name]>5
#           data = data.frame(exp_gDNA[selection, ])
#           plot_list[[name]] = ggplot(data, aes_string(paste0('log2(', name, ')')))+geom_histogram(binwidth=0.1)
#         }
#       }
#       do.call(grid.arrange, c(plot_list, ncol=2))
#     }
#   }
# }

# plot_list = list()
# for (poi in unique(exp_info$POI)){
#   for (day in unique(exp_info$day[exp_info$POI==poi])){
#     for (condition in unique(exp_info$condition)){
#       plot_list = list()
#       for (type in unique(exp_info$type)){
#         for (rep in unique(exp_info$replicate)){
#           name = colnames(exp_gDNA)[exp_info$POI==poi & exp_info$day == day &
#                                              exp_info$condition == condition &
#                                              exp_info$type == type &
#                                              exp_info$replicate == rep]
#           selection = exp_gDNA[,name]>5 & rownames(counts) %in% rownames(mapping)
#           data = data.frame(exp_gDNA[selection, ])
#           plot_list[[name]] = ggplot(exp_gDNA[selection, ], aes_string(paste0('log2(', name, ')')))+geom_histogram(binwidth=0.1)
#         }
#       }
#       do.call(grid.arrange, c(plot_list, ncol=2))
#     }
#   }
# }

```

```{r}
## calculate mean expression.
meanex <- function(replicates) {
  if (any(is.na(replicates))){
  	calc <- NA
  }
  else if (all(as.numeric(replicates) > 0)){
  	calc <- mean(replicates)
  }
  else {
    calc <- 0
  }
  return(calc)
}


## There has to be a better way to do these next two steps, but I can't think of anythin that makes sense from a reproducible point of view
rep_columns = tapply(1:nrow(exp_info), do.call(paste, c(exp_info[,colnames(exp_info)!='replicate'], sep='_')), I)
mean_exp = sapply(rep_columns, function(x, exp_gDNA){apply(exp_gDNA[,x],1,meanex)}, exp_gDNA)
mean_exp_info = exp_info[sapply(rep_columns,function(x){x[1]}),]
short_vec = c('GAL4.POI'='GP','POI'='P','GAL4'='G')
comb_matrix = combn(names(short_vec),2)

plot_vs <- function(poi, exp_matrix, exp_info){
	comb_matrix = combn(c('GAL4.POI', 'POI', 'GAL4'),2)
	result = c()
	for (day in unique(exp_info$day[exp_info$POI==poi])){
		day_result = apply(comb_matrix,2,
			function(condition_vec, day, poi, exp_matrix, exp_info){
				exp_data = cbind(exp_matrix[, exp_info$POI==poi & exp_info$day==day & exp_info$condition == condition_vec[1]], exp_matrix[, exp_info$POI==poi & exp_info$day==day & exp_info$condition == condition_vec[2]])
				exp_data = data.frame(exp_data)
				colnames(exp_data) = c('col1', 'col2')
        not_na = !is.na(exp_data[,2]) & !is.na(exp_data[,1])
				cor_line_1 = paste("r(p)=",signif(cor(x=exp_data[not_na, 'col2'], y=exp_data[not_na, 'col1'],method = "pearson", use="pairwise.complete.obs"),digits=3))
        cor_line_2 = paste0("r(s)=",signif(cor(x=exp_data[not_na,'col2'], y=exp_data[not_na,'col1'],method = "spearman",use="pairwise.complete.obs"),digits=3))
    	  condition_vec = str_replace(condition_vec,'[.]' ,'-')
    		condition_vec = str_replace(condition_vec, 'POI', poi)
    		label_vec = paste('log2(expr',condition_vec, '+ 1)')
				title = paste(condition_vec, collapse = ' vs ')
				title = paste(title, 'day', day)
				
				ggplot(exp_data[not_na,], aes(x=log2(col2 + 1), y=log2(col1 + 1))) +
					theme(panel.background = element_rect(fill = "lavender"))+
			    	geom_point(shape=19, size =0.8,colour="RED") +
			    	geom_abline()+
			    	#stat_smooth() +
			    	xlab(label_vec[2])+
			    	ylab(label_vec[1]) +
			    	#ggtitle("Gal4KRAB vs Gal4")+ 
			    	ggtitle(bquote(atop(.(title), atop(.(cor_line_1)), atop(.(cor_line_2)))))+ 
		    		theme(plot.title = element_text(size=24),text = element_text(size=20))
			}, day, poi, exp_matrix, exp_info)
		result = cbind(result, day_result)
	}
	return(result)
}


plot_replicates <- function(poi, exp_matrix, exp_info){
  condition_vec = c('GAL4.POI', 'POI', 'GAL4')
  result = c()
  for (day in unique(exp_info$day[exp_info$POI==poi])){
    day_result = lapply(condition_vec,
      function(condition, day, poi, exp_matrix, exp_info){
        exp_data = exp_matrix[, exp_info$POI==poi & exp_info$day==day & exp_info$condition == condition]
        exp_data = data.frame(exp_data)
        colnames(exp_data) = c('col1', 'col2')
        not_na = !is.na(exp_data[,2]) & !is.na(exp_data[,1])
        cor_line_1 = paste("r(p)=",signif(cor(x=exp_data[not_na, 'col2'], y=exp_data[not_na, 'col1'],method = "pearson", use="pairwise.complete.obs"),digits=3))
        cor_line_2 = paste0("r(s)=",signif(cor(x=exp_data[not_na,'col2'], y=exp_data[not_na,'col1'],method = "spearman",use="pairwise.complete.obs"),digits=3))
        condition = str_replace(condition,'[.]' ,'-')
        condition = str_replace(condition, 'POI', poi)
        label_vec = rep(paste('log2(expr',condition, '+ 1)'), 2)
        title = paste0(condition, ' rep1 vs rep2')
        title_2 = paste('day', day)
        
        ggplot(exp_data[not_na,], aes(x=log2(col2 + 1), y=log2(col1 + 1))) +
          theme(panel.background = element_rect(fill = "lavender"))+
            geom_point(shape=19, size =0.8,colour="RED") +
            geom_abline()+
            #stat_smooth() +
            xlab(label_vec[2])+
            ylab(label_vec[1]) +
            #ggtitle("Gal4KRAB vs Gal4")+ 
            ggtitle(bquote(atop(.(title), atop(.(title_2)), atop(.(cor_line_1)), atop(.(cor_line_2)))))+ 
            theme(plot.title = element_text(size=24),text = element_text(size=20))
      }, day, poi, exp_matrix, exp_info)
    result = cbind(result, day_result)
  }
  return(result)
}



```
```{r, fig.width=15, fig.height=19}
# do.call(grid.arrange, c(plot_replicates('KRAB', exp_gDNA, exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
```
```{r, fig.width=15, fig.height=15}
# do.call(grid.arrange, c(plot_replicates('G9a', exp_gDNA, exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
```
```{r, fig.width=15, fig.height=19}
# do.call(grid.arrange, c(plot_replicates('CBX', exp_gDNA, exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
```

```{r, fig.width=15, fig.height=19}
# do.call(grid.arrange, c(plot_vs('KRAB', mean_exp, mean_exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
```
```{r, fig.width=15, fig.height=15}
# do.call(grid.arrange, c(plot_vs('G9a', mean_exp, mean_exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
```
```{r, fig.width=15, fig.height=19}
# do.call(grid.arrange, c(plot_vs('CBX', mean_exp, mean_exp_info), top = "normalized mean expression (Christ's data)", ncol=3))
``` 

**conclusion:**
the samples from day 2 clearly show an effect of thethering any of the protein of interests,
but for KRAB this seems to be most clear. There seems to be some noise in each of the datasets, 
but there is no filtering done yet.


## Calculate foldchange and link locations and states
Now that we have expression values we can calculate fold changes and link these fold changes to chromosomal location, the lad/chromatin states and repetitive elements.

```{r, cache=F, fig.width=15, fig.height=6}
## calculate foldchange
## but leave out BCDs with zero reads
foldch <- function(exp_vec) {
  if (all(!is.na(exp_vec))) {
    calc <- as.numeric(exp_vec[1]) / as.numeric(exp_vec[2])
  }
  else {
    calc <- NA
  }
  return(calc)
}

apply_foldch <- function(cd_vec, mean_exp, mean_exp_info, short_vec){
	column_matrix = sapply(cd_vec,function(cd, info){
			this_info = info[which(cd==info$condition), colnames(info)!='replicate']
			col_vec = do.call(paste, c(this_info, sep='_'))
			names(col_vec) = paste0(this_info$POI, '_%s_day', this_info$day)
			return(col_vec)
		}, mean_exp_info)
    result = data.frame(apply(column_matrix, 1, function(col_name, mean_exp){
    	        apply(mean_exp[,col_name],1,foldch)
    		}, mean_exp))
   	names = paste(short_vec[cd_vec],collapse='vs')
   	this_exp_info = mean_exp_info[str_sub(colnames(result), 2,3),]
   	colnames(result) = do.call(sprintf, list(rownames(column_matrix), names))
    return(result)
}


fc_table = apply(comb_matrix,2, apply_foldch, mean_exp, mean_exp_info, short_vec)
fc_table = do.call(cbind, fc_table)

above_cut <- function(poi,norm_counts, info){
    poi_vec = which(info$POI==poi)
    day_vec = unique(info[poi_vec,'day'])
    out= sapply(day_vec, function(day, poi, norm_counts, info){
    	col_vec = which(info$day==day & info$POI==poi)
    	rowSums(norm_counts[,col_vec]>=50) == length(col_vec)
    	}, poi, norm_counts, info)
    colnames(out) = sprintf('%s_D%i_above_norm_cut',poi, day_vec)
    return(out)
    }

above_norm_cut = do.call(cbind, lapply(unique(info$POI), above_cut, counts_without_bc[,info$type=='norm'], info[info$type=='norm',]))
fc_table = cbind(fc_table, above_norm_cut)


map_match = match(rownames(fc_table), rownames(mapping))
fc_table = cbind(fc_table, mapping[map_match,])

discrete_types = c('chrom', 'lad', 'lad_2state', 'compartment', 'segment',
                   'rep_class', 'rep_family')
continuous_types = c('lad_size', 'lad_border_distance', 'cpg_distance', 'ctcf_distance', 'centromere_distance', 'telomere_distance', 'gene_distance', 'AT_ratio', 'lad_avg_probe', 'CBX1', 'CBX5', 'CBX8', 'CBX3_000', 'CBX3_163')
for (type in discrete_types){
  fc_table[[type]] = '-'
  fc_table[[paste0('unique_', type)]] = NA
}
for (type in continuous_types){
  fc_table[[type]] = NaN
  fc_table[[paste0('unique_', type)]] = NA
}

fam_match = match(rownames(fc_table), single_bc_rep_family$barcode)
class_match = match(rownames(fc_table), single_bc_rep_class$barcode)
lad_match = match(rownames(fc_table), single_bc_lad$barcode)
lad_2state_match = match(rownames(fc_table), single_bc_lad_2state$barcode)
lad_size_match = match(rownames(fc_table), single_bc_lad_size$barcode)
lad_border_match = match(rownames(fc_table), single_bc_lad_border$barcode)
compartment_match = match(rownames(fc_table), single_bc_subcompartments$barcode)

rep_segment_match = match(rownames(fc_table), single_bc_rep_segments$barcode)
chrom_match = match(rownames(fc_table), single_bc_chrom$barcode)
cpg_match = match(rownames(fc_table), single_bc_cpg_distance$barcode)
ctcf_match = match(rownames(fc_table), single_bc_ctcf_distance$barcode)
centromere_match = match(rownames(fc_table), single_bc_centromere_distance$barcode)
telomere_match = match(rownames(fc_table), single_bc_telomere_distance$barcode)
gene_match = match(rownames(fc_table), single_bc_gene_distance$barcode)
at_match = match(rownames(fc_table), single_bc_at_ratio$barcode)
lad_probe_match = match(rownames(fc_table), single_bc_lad_avg_probe$barcode)

CBX1_match = match(rownames(fc_table), single_bc_CBX1$barcode)
CBX5_match = match(rownames(fc_table), single_bc_CBX5$barcode)
CBX8_match = match(rownames(fc_table), single_bc_CBX8$barcode)
CBX3_000_match = match(rownames(fc_table), single_bc_CBX3_000$barcode)
CBX3_163_match = match(rownames(fc_table), single_bc_CBX3_163$barcode)


fc_table[!is.na(fam_match),c('rep_family', 'unique_rep_family')] = single_bc_rep_family[fam_match[!is.na(fam_match)], c('family', 'unique_rep_family')]
fc_table[!is.na(class_match),c('rep_class', 'unique_rep_class')] = single_bc_rep_class[class_match[!is.na(class_match)], c('class', 'unique_rep_class')]
fc_table[!is.na(lad_match),c('lad', 'unique_lad')] = single_bc_lad[lad_match[!is.na(lad_match)], c('lad', 'unique_lad')]
fc_table[!is.na(lad_2state_match),c('lad_2state', 'unique_lad_2state')] = single_bc_lad_2state[lad_2state_match[!is.na(lad_2state_match)], c('lad', 'unique_lad')]
fc_table[!is.na(lad_size_match),c('lad_size', 'unique_lad_size')] = single_bc_lad_size[lad_size_match[!is.na(lad_size_match)], c('lad_size', 'unique_size')]
fc_table[!is.na(lad_border_match),c('lad_border_distance', 'unique_lad_border_distance')] = single_bc_lad_border[lad_border_match[!is.na(lad_border_match)], c('distance', 'unique_distance')]
fc_table[!is.na(compartment_match),c('compartment', 'unique_compartment')] = single_bc_subcompartments[compartment_match[!is.na(compartment_match)], c('compartment', 'unique_compartment')]

fc_table[!is.na(rep_segment_match),c('segment', 'unique_segment')] = single_bc_rep_segments[rep_segment_match[!is.na(rep_segment_match)], c('segment', 'unique_segment')]
fc_table[!is.na(chrom_match),c('chrom', 'unique_chrom')] = single_bc_chrom[chrom_match[!is.na(chrom_match)], c('chrom', 'unique_chrom')]
fc_table[!is.na(cpg_match),c('cpg_distance', 'unique_cpg_distance')] = single_bc_cpg_distance[cpg_match[!is.na(cpg_match)], c('distance', 'unique_cpg_distance')]
fc_table[!is.na(ctcf_match),c('ctcf_distance', 'unique_ctcf_distance')] = single_bc_ctcf_distance[ctcf_match[!is.na(ctcf_match)], c('distance', 'unique_ctcf_distance')]
fc_table[!is.na(centromere_match),c('centromere_distance', 'unique_centromere_distance')] = single_bc_centromere_distance[centromere_match[!is.na(centromere_match)], c('distance', 'unique_centromere_distance')]
fc_table[!is.na(telomere_match),c('telomere_distance', 'unique_telomere_distance')] = single_bc_telomere_distance[telomere_match[!is.na(telomere_match)], c('distance', 'unique_telomere_distance')]
fc_table[!is.na(gene_match),c('gene_distance', 'unique_gene_distance')] = single_bc_gene_distance[gene_match[!is.na(gene_match)], c('distance', 'unique_gene_distance')]
fc_table[!is.na(at_match),c('AT_ratio', 'unique_AT_ratio')] = single_bc_at_ratio[at_match[!is.na(at_match)], c('AT_ratio', 'unique_AT_ratio')]
fc_table[!is.na(lad_probe_match),c('lad_avg_probe', 'unique_lad_avg_probe')] = single_bc_lad_avg_probe[lad_probe_match[!is.na(lad_probe_match)], c('avg_signal', 'unique_avg_signal')]

fc_table[!is.na(CBX1_match),c('CBX1', 'unique_CBX1')] = single_bc_CBX1[CBX1_match[!is.na(CBX1_match)], c('CBX1', 'unique_CBX1')]
fc_table[!is.na(CBX5_match),c('CBX5', 'unique_CBX5')] = single_bc_CBX5[CBX5_match[!is.na(CBX5_match)], c('CBX5', 'unique_CBX5')]
fc_table[!is.na(CBX8_match),c('CBX8', 'unique_CBX8')] = single_bc_CBX8[CBX8_match[!is.na(CBX8_match)], c('CBX8', 'unique_CBX8')]
fc_table[!is.na(CBX3_000_match),c('CBX3_000', 'unique_CBX3_000')] = single_bc_CBX3_000[CBX3_000_match[!is.na(CBX3_000_match)], c('CBX3_000', 'unique_CBX3_000')]
fc_table[!is.na(CBX3_163_match),c('CBX3_163', 'unique_CBX3_163')] = single_bc_CBX3_163[CBX3_163_match[!is.na(CBX3_163_match)], c('CBX3_163', 'unique_CBX3_163')]


for (fase in timing_fases){
  name = paste0('signal_', fase)
  unique = paste0('unique_', name)
  fc_table[[name]] = NaN
  fc_table[[unique]] = NA
  match = match(rownames(fc_table), single_bc_timing[[fase]]$barcode)
  fc_table[!is.na(match),c(name, unique)] = single_bc_timing[[fase]][match[!is.na(match)], c(name, unique)]
}

plot_unique_vs <- function(mean_exp, fc_table, info, top, unique_map=T){
	for (poi in unique(info$POI)){
		for (day in sort(unique(info[info$POI==poi,'day']))){
			col_name = sprintf('%s_D%i_above_norm_cut', poi, day)
			if (unique_map){
				selection = fc_table[,col_name]&fc_table$unique_map
			} else { 
				selection = fc_table[,col_name]
			}
			plots = plot_vs(poi, mean_exp[which(selection),info$day==day], info[info$day==day,])
			do.call(grid.arrange, c(plots[c(2,3,1)], top = top, nrow=1))
		}
	}
}


# plot_unique_vs(mean_exp, fc_table, mean_exp_info, "normalized mean expression filtered on  normalization counts (Christ's data)", unique_map=F)

# plot_unique_vs(mean_exp, fc_table, mean_exp_info, "normalized mean expression filtered on unique mapping and normalization (Christ's data)")


```
**conclusion:**
I managed to do the same with my data as Laura did with Laura's. There is only one difference in the pipeline due to how starcode (the program I use to seperate genuine barcodes from mutated ones) reports it's data. With previous trip pipeline expression counts got added to the genuine barcode only if this was the only available barcode in the barcode list within the Levenhstein distance. So barcodes that had 2 possible genuine ones got discarded. Starcode however does not report these instances different from barcodes that have just one possibility since it always choses the 'best' genuine barcode. 


### find barcodes with multiple mapping locations.
to make a better comparison between the barcode-state/repeat links and barcode-location links, I added a plot filtering on reverse reads only, since this approach was also used in the other links.

```{r, fig.width=10, fig.height=10}

unique_rev = rep(F, nrow(mapping))
isUnique = which(mapping$t_reads_r>2 &mapping$freq1_r>0.7 & mapping$freq2_r<0.1 & mapping$mapq_f>=10)
unique_rev[isUnique] = T

mapping_count = ddply(mapping, .(unique_map), summarize, y=length(unique_map))
mapping_count_rev = ddply(data.frame('unique_map_rev'=unique_rev), .(unique_map_rev), summarize, y=length(unique_map_rev))
ymax = max(c(mapping_count_rev$y, mapping_count_rev$y, unique_repeat_name_count$y, unique_repeat_fam_count$y, unique_repeat_class_count$y, unique_lad_count$y, unique_chrom_count$y, unique_cpg_count$y))

unique_class = data.frame(map=rep(NA,nrow(fc_table)),
                          rep_name=rep(NA, nrow(fc_table)),
                          rep_fam=rep(NA,nrow(fc_table)),
                          rep_class=rep(NA, nrow(fc_table)),
                          lad=rep(NA, nrow(fc_table)),
                          chrom=rep(NA, nrow(fc_table)),
                          cpg_distance=rep(NA, nrow(fc_table)))

unique_class$rep_name[fc_table$rep_name!='-'] = 'not unique'
unique_class$rep_name[fc_table$unique_rep_name] = 'unique family/state'
unique_class$rep_name[fc_table$unique_map&fc_table$rep_name!='-'] = 'unique location'

unique_class$rep_fam[fc_table$rep_family!='-'] = 'not unique'
unique_class$rep_fam[fc_table$unique_rep_family] = 'unique family/state'
unique_class$rep_fam[fc_table$unique_map&fc_table$rep_family!='-'] = 'unique location'

unique_class$rep_class[fc_table$rep_class!='-'] = 'not unique'
unique_class$rep_class[fc_table$unique_rep_class] = 'unique family/state'
unique_class$rep_class[fc_table$unique_map&fc_table$rep_class!='-'] = 'unique location'

unique_class$lad[fc_table$lad!='-'] = 'not unique'
unique_class$lad[fc_table$unique_lad&fc_table$lad!='-'] = 'unique family/state'
unique_class$lad[fc_table$unique_map&fc_table$lad!='-'] = 'unique location'

unique_class$chrom[fc_table$chrom!='-'] = 'not unique'
unique_class$chrom[fc_table$unique_chrom] = 'unique family/state'
unique_class$chrom[fc_table$unique_map&fc_table$chrom!='-'] = 'unique location'

unique_class$cpg_distance[fc_table$cpg_distance!='-'] = 'not unique'
unique_class$cpg_distance[fc_table$unique_cpg_distance] = 'unique family/state'
unique_class$cpg_distance[fc_table$unique_map&fc_table$cpg_distance!='-'] = 'unique location'

for (fase in timing_fases){
  name = paste0('signal_', fase)
  unique = paste0('unique_', name)
  unique_class[[name]] = NA
  unique_class[[name]][fc_table[,name]!='-'] = 'not unique'
  unique_class[[name]][fc_table[,unique]] = 'unique family/state'
  unique_class[[name]][fc_table[,unique] & fc_table$unique_map] = 'unique location'
}

unique_class$map[!fc_table$unique_map] = 'not unique'
unique_class$map[fc_table$unique_map] = 'unique location'
colnames(unique_class) = c('mapping', 'repeat name', 'repeat family', 'repeat class', 'LAD-state', 'Chromatin-state', 'cpg distance', paste('Replication timing', timing_fases))

# unique_table = melt(cbind('barcode'=rownames(fc_table),unique_class),id.vars='barcode')
# ggplot(unique_table[!is.na(unique_table$value),], aes(x=factor(variable), fill=factor(value, levels=c('unique location', 'unique family/state', 'not unique')))) + geom_bar() + 
#     ylab('count') +
#     xlab('datatype to which the barcode is linked') + 
#     theme(axis.text.x = element_text(hjust = 1, angle = 90)) +
#     ggtitle('number of barcodes linked to\ndifferent sources of information') +
#     scale_fill_discrete(name = 'unique?') 

```

**Conclusion:**

There is a relatively small amount of barcodes with multiple hits left for LAD states and chromatin states with multiple hits after Laura's filtering. These should overlap with the barcodes mapped at different locations.

The barcodes mapped at different locations however show a lot more multiple locations. Only partly because the selection is more stringent since the forward read is taken into account.



## Save fold change data and mean expressions


```{r}

f_name = '../results/TTRIP_K562_FC_exp.rData'
fc_table = cbind(exp_gDNA, mean_exp, fc_table)
save(fc_table, mean_exp_info, file=f_name)

## Session Info
sessionInfo()
getwd()
date()
paste("Run time: ",format(Sys.time()-StartTime))
```
